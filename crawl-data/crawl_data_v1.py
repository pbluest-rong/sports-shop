import requests
from bs4 import BeautifulSoup
import json
import time
import re
from bson import ObjectId
from slugify import slugify

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

BASE_URL = "https://samba.vn"

# ----------- Danh s√°ch category mapping -----------
CATEGORY_MAPPING = {
    "quan-ao-bong-da": "64f1d2f77bcf86cd79943901",
    "gym-yoga": "64f1d2f77bcf86cd79943902",
    "giay-chay-bo": "64f1d2f77bcf86cd79943903",
    "quan-ao-the-thao": "64f1d2f77bcf86cd79943904",
    "pickleball": "64f1d2f77bcf86cd79943905",
    "quan-ao-bong-chuyen": "64f1d2f77bcf86cd79943906",
    "bong-ro": "64f1d2f77bcf86cd79943907",
    "do-boi": "64f1d2f77bcf86cd79943908",
    "bong-ban": "64f1d2f77bcf86cd79943909"
}

# ----------- H√†m parse gi√° v·ªÅ int -----------
def parse_price(price_text):
    if not price_text:
        return 0
    try:
        return int(re.sub(r"\D", "", price_text))  # lo·∫°i b·ªè k√Ω t·ª± kh√¥ng ph·∫£i s·ªë
    except:
        return 0

# ----------- L·∫•y danh s√°ch link s·∫£n ph·∫©m -----------
def get_product_links(category_url):
    try:
        response = requests.get(category_url, headers=HEADERS)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        products = soup.select("h3.product-name a")

        links = []
        for a in products:
            href = a.get("href")
            if href and not href.startswith("http"):
                href = BASE_URL + href
            links.append(href)
        return links
    except Exception as e:
        print(f"L·ªói khi l·∫•y link s·∫£n ph·∫©m: {e}")
        return []

# ----------- L·∫•y chi ti·∫øt s·∫£n ph·∫©m (Fix nh·∫ßm l·∫´n) -----------
def get_product_detail(url):
    try:
        res = requests.get(url, headers=HEADERS)
        res.raise_for_status()
        res.encoding = "utf-8"
        soup = BeautifulSoup(res.text, "html.parser")

        # T√™n s·∫£n ph·∫©m
        title = soup.select_one("h1.title-product").get_text(strip=True)

        # Gi√° s·∫£n ph·∫©m
        price_element = soup.select_one("span.price.product-price")
        price = price_element.get_text(strip=True) if price_element else "0"

        # M√¥ t·∫£
        description = soup.select_one(".product-summary .rte").get_text(" ", strip=True) if soup.select_one(".product-summary .rte") else ""

        # ·∫¢nh s·∫£n ph·∫©m
        images = ["https:" + a["href"] for a in soup.select("#lightgallery a")]
        images.reverse()

        # Th∆∞∆°ng hi·ªáu v√† t√¨nh tr·∫°ng
        brand = soup.select_one(".a-vendor").get_text(strip=True) if soup.select_one(".a-vendor") else ""
        status = soup.select_one(".a-stock").get_text(strip=True) if soup.select_one(".a-stock") else "C√≤n h√†ng"

        # Map t√™n thu·ªôc t√≠nh (color / size) t·ª´ HTML
        option_labels = {}
        for opt in soup.select('.swatch'):
            idx = opt.get('data-option-index')
            label = opt.select_one('.header').get_text(strip=True).lower() if opt.select_one('.header') else ""
            if "m√†u" in label:
                option_labels[idx] = "color"
            elif "size" in label or "k√≠ch" in label:
                option_labels[idx] = "size"

        # Map m·ªói m√†u v·ªõi 2 ·∫£nh li√™n ti·∫øp
        color_image_map = {}
        image_index = 0

        # Danh s√°ch variants
        variants = []
        for option in soup.select("#product-selectors option"):
            text = option.get_text(strip=True)
            value = option.get("value")
            if " - " in text:
                parts = text.split(" - ")
                attr_values = parts[0].split(" / ")
                price_variant = parse_price(parts[1].strip())

                # G√°n color/size theo option_labels
                attrs = {}
                for idx, val in enumerate(attr_values):
                    key = option_labels.get(str(idx), "")
                    if key and val.strip():
                        attrs[key] = val.strip()

                # Map ·∫£nh theo m√†u (n·∫øu c√≥ color)
                color = attrs.get("color")
                if color and color not in color_image_map:
                    color_image_map[color] = images[image_index:image_index+2]
                    image_index += 2

                variant_images = color_image_map.get(color, images)

                variants.append({
                    "variant_id": value,
                    "attributes": attrs,
                    "price": price_variant,
                    "availableStock": 0,
                    "images": variant_images
                })

        return {
            "title": title,
            "price": price,
            "description": description,
            "brand": brand,
            "status": "ACTIVE",
            "images": images,
            "variants": variants,
            "url": url
        }

    except Exception as e:
        print(f"L·ªói khi l·∫•y chi ti·∫øt s·∫£n ph·∫©m {url}: {e}")
        return None

# ----------- Chu·∫©n h√≥a d·ªØ li·ªáu ƒë·ªÉ l∆∞u MongoDB -----------
def normalize_product(detail, category_slug):
    slug = slugify(detail.get("title", ""))

    # L·∫•y category_id t·ª´ mapping
    category_id = CATEGORY_MAPPING.get(category_slug, "507f1f77bcf86cd799439012")

    # Parse gi√° ch√≠nh
    main_price = parse_price(detail.get("price", 0))

    normalized = {
        "_id": str(ObjectId()),
        "slug": slug,
        "title": detail.get("title", ""),
        "description": detail.get("description", ""),
        "brand": detail.get("brand", ""),
        "status": detail.get("status", "C√≤n h√†ng"),
        "sold": 10,
        "categoryId": {"$oid": category_id},
        "attributes": {
            "colors": [],
            "sizes": []
        },
        "variants": [],
        "rating": {
            "avg": 0,
            "count": 0
        },
        "createdAt": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "updatedAt": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
    }

    variants = []
    colors_set = set()
    sizes_set = set()

    for v in detail.get("variants", []):
        attrs = v.get("attributes", {})
        color = attrs.get("color")
        size = attrs.get("size")

        if color:
            colors_set.add(color)
        if size:
            sizes_set.add(size)

        variants.append({
            "sku": f"SKU-{v.get('variant_id', '')}",
            "id": v.get("variant_id", ""),
            "attributes": attrs,
            "price": v.get("price", 0),
            "availableStock": v.get("availableStock", 0),
            "images": v.get("images", detail.get("images", []))
        })

    # N·∫øu kh√¥ng c√≥ variants, t·∫°o m·∫∑c ƒë·ªãnh
    if not variants:
        variant_id = str(int(time.time() * 1000))
        variants.append({
            "sku": f"SKU-{variant_id}",
            "id": variant_id,
            "attributes": {},
            "price": main_price,
            "availableStock": 10 if detail.get("status") == "C√≤n h√†ng" else 0,
            "images": detail.get("images", [])
        })

    product_attributes = {}
    if colors_set:
        product_attributes["colors"] = list(colors_set)
    if sizes_set:
        product_attributes["sizes"] = list(sizes_set)

    normalized["attributes"] = product_attributes
    normalized["variants"] = variants

    return normalized

# ----------- Crawl theo danh m·ª•c -----------
def crawl_category(category_url, category_slug, start_page=1, end_page=1):
    all_products = []
    for page in range(start_page, end_page + 1):
        page_url = f"{category_url}?page={page}"
        print(f"ƒêang crawl trang: {page_url}")
        product_links = get_product_links(page_url)
        print(f"  -> T√¨m th·∫•y {len(product_links)} s·∫£n ph·∫©m")

        for link in product_links:
            detail = get_product_detail(link)
            if detail:
                normalized = normalize_product(detail, category_slug)
                all_products.append(normalized)
                print(f"   ‚úì ƒê√£ crawl: {normalized['title']} - Category: {category_slug}")

    return all_products

# ----------- Ch·∫°y crawl v√† l∆∞u file -----------
c1 = "quan-ao-bong-da"
c2 = "gym-yoga"
c3 = "giay-chay-bo"
c4 = "quan-ao-the-thao"
c5 = "pickleball"
c6 = "quan-ao-bong-chuyen"
c7 = "bong-ro"
c8 = "do-boi"
c9 = "bong-ban"

category_slug = c9
CATEGORY_URL = f"https://samba.vn/{category_slug}"

products = crawl_category(CATEGORY_URL, category_slug, start_page=1, end_page=1)

with open("products/" + category_slug + ".json", "w", encoding="utf-8") as f:
    json.dump(products, f, indent=4, ensure_ascii=False)

print(f"\n‚úÖ ƒê√£ l∆∞u {len(products)} s·∫£n ph·∫©m v√†o " + category_slug + ".json")
print(f"üìÅ Category: {category_slug} - ID: {CATEGORY_MAPPING[category_slug]}")